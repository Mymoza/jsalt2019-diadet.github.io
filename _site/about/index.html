<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>About our workshop | Speaker Detection in Adverse Scenarios with a Single Microphone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="About our workshop" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We are a group of researchers meeting together to solve speaker detection in adverse scenarios." />
<meta property="og:description" content="We are a group of researchers meeting together to solve speaker detection in adverse scenarios." />
<link rel="canonical" href="http://localhost:4000/about/" />
<meta property="og:url" content="http://localhost:4000/about/" />
<meta property="og:site_name" content="Speaker Detection in Adverse Scenarios with a Single Microphone" />
<script type="application/ld+json">
{"name":"Speaker Detection in Adverse Scenarios with a Single Microphone","description":"We are a group of researchers meeting together to solve speaker detection in adverse scenarios.","@type":"WebSite","url":"http://localhost:4000/about/","headline":"About our workshop","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Speaker Detection in Adverse Scenarios with a Single Microphone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Speaker Detection in Adverse Scenarios with a Single Microphone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About our workshop</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">About our workshop</h1>
  </header>

  <div class="post-content">
    <p>Introduction</p>

<p>Devices that document daily life experiences could provide a unique view on the user’s spontaneous speech use. For the past years, speaker recognition research, has mainly focused on telephone and close microphone applications with high speech quality levels. However, far-field recordings have recently emerged as an area of interest. This is due to new applications like annotating the videos in platforms as Facebook and Youtube; home assistant devices which need to distinguish between family members; and wearables that document our everyday life. These applications provide a massive amount of data which requires automatic means of analysis. Furthermore, such devices are often used in very challenging environments with multiple speakers, and where the audio is affected by noise and reverberation.
Most devices use a single microphone and, therefore, multichannel signal processing techniques (e.g., beamforming) cannot be applied to alleviate the impact of the real-life conditions. As reference, speaker detection error rates of reverberant speech are 2 times worse than close talk speech in voices dataset; internet videos with noise and multi-speaker error multiplied by 6 in recent NIST SRE18 w.r.t. clean videos;  and systematic evaluation of commercial diarization on wearables in real-life domain shows diarization error rates of about 60%. The aim of this proposal is to research, develop, and benchmark speaker diarization and speaker recognition systems on far-field speech acquired using single microphones in realistic scenarios that include background noises such as a television audio, music, or other people talking (as shown in Figure 1).</p>

<p>Figure 1. Speaker Detection</p>

<p>To get plausible conclusions, we will explore two complementary scenarios. The first one focuses on acoustically challenging environments with known conditions. The information of the room, noise types, and the position of the microphone will be provided so that each problem can be isolated and studied. The other scenario pertains to day-long recordings from child-worn devices. This is a particular innovation of the project because samples from such recordings portray a broad array of room conditions (including outdoors), noise types, speaker similarity, and other adverse conditions (such as long silences alternating with speech bursts by multiple, potentially overlapping talkers) and furthermore, these conditions are not known. This will allow us to assess to what extent highly informed algorithmic solutions generalize across a range of real-life situations. Furthermore, these two well-defined scenarios will allow more accurate measurements of the performance and generalization level of speaker detection and diarization algorithms.</p>

<p>Research Threads
Research areas for the proposed workshop include:</p>

<p>Figure 2. Research Threads</p>

<p>SAD/Speech Classification: we require robust speech detection. This involves ascertaining that speech is produced by real people in the room or from other sources like HVAC, babble, traffic, TV/radio speech, and music; and that child speech and other immature vocalizations are preserved (rather than treated as noise).</p>

<p>Figure 3. SAD output</p>

<p>Speech enhancement: we need to compensate for the effect of noise and reverberation in far-field audio. For this work, we intend to follow two research tracks for which we already have some preliminary results. The first one is supervised training of neural speech enhancement–given clean-noisy paired data. The focus will be to extend this method to train the enhancement block in combination with the speaker embedding network. The second track is unsupervised training of enhancement using CycleGAN. Special care will be taken to keep the children speech which would otherwise be considered a nuisance for some applications.</p>

<p>Figure 4. Spectrum from noisy to enhanced speech</p>

<p>Speaker Diarization/Tracking: Dealing with the problem of detecting the target speakers within a conversation of multiple other speakers (children-adult scenario, often with overlap.</p>

<p>Figure 5. Diarization (1 is the label for speaker1, 2 is the label for speaker2, blank space is silence)</p>

<p>Domain Adaptation: Adaptation of speaker recognition systems using adversarial training to mitigate expected (but unknown) differences between system training data and the evaluation conditions (e.g. indoor vs. outdoor, controlled vs. adverse). Calibration in far-field conditions will also be of interest since different distances and pathways between the source and the microphone affect the distribution of the scores. Calibration approaches will need to compensate for dynamic changes in the environment, such as movement of the source.</p>

<p>Figure 5. Domain adaptation</p>

<p>Expected Outcomes
As a result of the JSALT workshop, the speech community, industry, and academia, will benefit from: a better understanding of single-microphone far-field speech in real-life scenarios with realistic background noises for speaker detection and diarization task; novel machine learning and signal processing algorithms; and a complete benchmarking of single-microphone realistic databases. Moreover, the research outcomes as papers and shareable code will be a priority for the group.<br />
We believe that the proposed workshop will provide a concentrated effort to address many of the outstanding research problems in the analysis of speaker recognition techniques using distant speech with single microphones in realistic scenarios and motivate future research in the community.</p>

<!-- This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at [jekyllrb.com](https://jekyllrb.com/)

#You can find the source code for Minima at GitHub:
#[jekyll][jekyll-organization] /
#[minima](https://github.com/jekyll/minima)

#You can find the source code for Jekyll at GitHub:
#[jekyll][jekyll-organization] /
#[jekyll](https://github.com/jekyll/jekyll)


#[jekyll-organization]: https://github.com/jekyll -->

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Speaker Detection in Adverse Scenarios with a Single Microphone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Speaker Detection in Adverse Scenarios with a Single Microphone</li><li><a class="u-email" href="mailto:leibny at gmail.com">leibny at gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jsalt2019-diadet"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jsalt2019-diadet</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>We are a group of researchers meeting together to solve speaker detection in adverse scenarios. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
